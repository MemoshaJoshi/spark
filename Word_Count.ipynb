{"cells":[{"cell_type":"markdown","source":["# Word Count\n\n### Counting the number of occurances of words in a text is a popular first exercise using map-reduce.\n\n## The Task\n**Input:** A text file consisisting of words separated by spaces.  \n**Output:** A list of words and their counts, sorted from the most to the least common.\n\nWe will use the book \"Moby Dick\" as our input."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab1fe082-688a-4caa-bf9b-707baeb49f7d"}}},{"cell_type":"code","source":["#start the SparkContext\nfrom pyspark import SparkContext\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4962fa25-506f-40f7-ba66-d38b784418de"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc=SparkContext(master=\"local[4]\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26c563a8-3460-4557-871c-9e50edf4bd17"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-3704061629036308>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmaster\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"local[4]\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    357\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 358\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    359\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-3704061629036308>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmaster\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"local[4]\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    143\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    356\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    357\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 358\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    359\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    360\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "]}}],"execution_count":0},{"cell_type":"markdown","source":["### Setup a plan for pretty print"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60425cad-0cd6-4082-b319-41f41e773cc3"}}},{"cell_type":"code","source":["def pretty_print_plan(rdd):\n    for x in rdd.toDebugString().decode().split('\\n'):\n        print(x)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"deffd215-950b-45e0-9434-adb395851e48"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Use `textFile()` to read the text"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4edb8078-e87a-4e2e-a4b4-eb1f8dd2f902"}}},{"cell_type":"code","source":["%%time\ntext_file = sc.textFile(\"/FileStore/tables/Moby_Dick.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d1cf108-e7b8-4bc5-b851-bd7127cd011a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"CPU times: user 773 µs, sys: 203 µs, total: 976 µs\nWall time: 74.8 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CPU times: user 773 µs, sys: 203 µs, total: 976 µs\nWall time: 74.8 ms\n"]}}],"execution_count":0},{"cell_type":"code","source":["type(text_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b43f08a-af54-4542-9863-0d1bc318b209"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: pyspark.rdd.RDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: pyspark.rdd.RDD"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Steps for counting the words\n\n* split line by spaces.\n* map `word` to `(word,1)`\n* count the number of occurances of each word."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ef1c245-df74-49b0-a924-67d045a3bdc4"}}},{"cell_type":"code","source":["%%time\nwords =text_file.flatMap(lambda line: line.split(\" \"))\nnot_empty = words.filter(lambda x: x!='') \nkey_values= not_empty.map(lambda word: (word, 1)) \nkey_values.collect()\ncounts=key_values.reduceByKey(lambda a, b: a + b)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f703e4c-89ec-4c63-8c56-b5839dc5be2e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"CPU times: user 90.7 ms, sys: 16.7 ms, total: 107 ms\nWall time: 1.65 s\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CPU times: user 90.7 ms, sys: 16.7 ms, total: 107 ms\nWall time: 1.65 s\n"]}}],"execution_count":0},{"cell_type":"code","source":["counts.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32667b90-2d91-4461-b497-047929aba70b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[13]: [('The', 549), ('Project', 79), ('EBook', 1), ('of', 6587), ('Moby', 79)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: [('The', 549), ('Project', 79), ('EBook', 1), ('of', 6587), ('Moby', 79)]"]}}],"execution_count":0},{"cell_type":"markdown","source":["### flatMap()\nNote the line:\n```python\nwords =     text_file.flatMap(lambda line: line.split(\" \"))\n```\nWhy are we using `flatMap`, rather than `map`?\n\nThe reason is that the operation `line.split(\" \")` generates a **list** of strings, so had we used `map` the result would be an RDD of lists of words. Not an RDD of words.\n\nThe difference between `map` and `flatMap` is that the second expects to get a list as the result from the map and it **concatenates** the lists to form the RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"842c7a71-bc76-418a-ae39-4d0718af9788"}}},{"cell_type":"markdown","source":["## The execution plan\nIn the last cell we defined the execution plan, but we have not started to execute it.\n\n* Preparing the plan took ~100ms, which is a non-trivial amount of time, \n* But much less than the time it will take to execute it.\n* Lets have a look a the execution plan."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2f76fbf-3870-438f-a8f3-f5c0c582c368"}}},{"cell_type":"markdown","source":["### Understanding the details\nTo see which step in the plan corresponds to which RDD we print out the execution plan for each of the RDDs.  \n\nNote that the execution plan for `words`, `not_empty` and `key_values` are all the same."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c26b24d3-c5ed-4457-9198-69a4bd4da6c2"}}},{"cell_type":"code","source":["pretty_print_plan(text_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bcf9567-913c-4add-84ee-3b82fb4206b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(2) /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(2) /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n"]}}],"execution_count":0},{"cell_type":"code","source":["pretty_print_plan(words)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24a39580-1451-4346-ba85-b8d99ddc33d9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(2) PythonRDD[16] at RDD at PythonRDD.scala:58 []\n |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(2) PythonRDD[16] at RDD at PythonRDD.scala:58 []\n |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n"]}}],"execution_count":0},{"cell_type":"code","source":["pretty_print_plan(not_empty)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cad6829-f668-4aad-adc6-ad0e586eca24"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(2) PythonRDD[17] at RDD at PythonRDD.scala:58 []\n |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(2) PythonRDD[17] at RDD at PythonRDD.scala:58 []\n |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n"]}}],"execution_count":0},{"cell_type":"code","source":["pretty_print_plan(key_values)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4210a61-cd0d-45a1-87d6-f3c3f4356d4f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(2) PythonRDD[10] at collect at <timed exec>:4 []\n |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(2) PythonRDD[10] at collect at <timed exec>:4 []\n |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n"]}}],"execution_count":0},{"cell_type":"code","source":["pretty_print_plan(counts)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf84029e-a0fb-43a3-bf61-8473a763b1bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"(2) PythonRDD[18] at RDD at PythonRDD.scala:58 []\n |  MapPartitionsRDD[14] at mapPartitions at PythonRDD.scala:183 []\n |  ShuffledRDD[13] at partitionBy at NativeMethodAccessorImpl.java:0 []\n +-(2) PairwiseRDD[12] at reduceByKey at <timed exec>:5 []\n    |  PythonRDD[11] at reduceByKey at <timed exec>:5 []\n    |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n    |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["(2) PythonRDD[18] at RDD at PythonRDD.scala:58 []\n |  MapPartitionsRDD[14] at mapPartitions at PythonRDD.scala:183 []\n |  ShuffledRDD[13] at partitionBy at NativeMethodAccessorImpl.java:0 []\n +-(2) PairwiseRDD[12] at reduceByKey at <timed exec>:5 []\n    |  PythonRDD[11] at reduceByKey at <timed exec>:5 []\n    |  /FileStore/tables/Moby_Dick.txt MapPartitionsRDD[9] at textFile at NativeMethodAccessorImpl.java:0 []\n    |  /FileStore/tables/Moby_Dick.txt HadoopRDD[8] at textFile at NativeMethodAccessorImpl.java:0 []\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["| Execution plan   | RDD |  Comments |\n| :---------------------------------------------------------------- | :------------: | :--- |\n|`(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 []`| **counts** | Final RDD|\n|`_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []`| **---\"---** |\n|`_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [`| **---\"---** | RDD is partitioned by key |\n|`_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []`| **---\"---** | Perform mapByKey |\n|`____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []`| **words, not_empty, key_values** | The result of  partitioning into words|\n| | |  removing empties, and making into (word,1) pairs|\n|`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| **text_file** | The partitioned text |\n|`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| **---\"---** | The text source |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1de6cd89-46ee-4ab1-b933-2f12ca7dd010"}}},{"cell_type":"markdown","source":["## Execution\nFinally we count the number of times each word has occured.\nNow, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b907d6d6-3f4d-4501-aa2e-7947c13e82b2"}}},{"cell_type":"code","source":["%%time\n## Run #1\nCount=counts.count()  # Count = the number of different words\nSum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y) # \nprint('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f06f0f64-220c-4aa9-9911-4d21e9da3216"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 15.4 ms, sys: 1.97 ms, total: 17.3 ms\nWall time: 566 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 15.4 ms, sys: 1.97 ms, total: 17.3 ms\nWall time: 566 ms\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Amortization\nWhen the same commands are performed repeatedly on the same data, the execution time tends to decrease in later executions.\n\nThe cells below are identical to the one above, with one exception at `Run #3`\n\nObserve that `Run #2` take much less time that `Run #1`. Even though no `cache()` was explicitly requested. The reason is that Spark caches (or materializes) `key_values`, before executing `reduceByKey()` because performng reduceByKey requires a shuffle, and a shuffle requires that the input RDD is materialized. In other words, sometime caching happens even if the programmer did not ask for it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a295a077-10ca-42d1-95ca-77836869d5ae"}}},{"cell_type":"code","source":["%%time\n## Run #2\nCount=counts.count()\nSum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\nprint('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4cb88b0-f983-43af-9fd6-683bbc9a852c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 13.3 ms, sys: 395 µs, total: 13.7 ms\nWall time: 422 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 13.3 ms, sys: 395 µs, total: 13.7 ms\nWall time: 422 ms\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Explicit Caching\nIn `Run #3` we explicitly ask for `counts` to be cached. This will reduce the execution time in the following run `Run #4` by a little bit, but not by much."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3b976a5-2dd3-4400-b980-0a396cc2cad6"}}},{"cell_type":"code","source":["%%time\n## Run #3, cache\nCount=counts.cache().count()\nSum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\nprint('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b52ea50-2e81-47e7-9980-614045c12378"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 11.9 ms, sys: 3.08 ms, total: 15 ms\nWall time: 625 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 11.9 ms, sys: 3.08 ms, total: 15 ms\nWall time: 625 ms\n"]}}],"execution_count":0},{"cell_type":"code","source":["%%time\n#Run #4\nCount=counts.count()\nSum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\nprint('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98408a3f-cd12-42c3-bd8e-3871a13f934b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 7.79 ms, sys: 4.87 ms, total: 12.7 ms\nWall time: 295 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 7.79 ms, sys: 4.87 ms, total: 12.7 ms\nWall time: 295 ms\n"]}}],"execution_count":0},{"cell_type":"code","source":["%%time\n#Run #5\nCount=counts.count()\nSum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\nprint('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d65de35-9e1c-47af-a67e-d4f8c48333eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 8.87 ms, sys: 3.7 ms, total: 12.6 ms\nWall time: 315 ms\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Different words=33781, total words=215133, mean no. occurances per word=6.37\nCPU times: user 8.87 ms, sys: 3.7 ms, total: 12.6 ms\nWall time: 315 ms\n"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.10.1","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"Word_Count","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3704061629036305}},"nbformat":4,"nbformat_minor":0}
